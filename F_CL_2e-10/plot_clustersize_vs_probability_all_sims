#!/home/vsivadas/anaconda/bin/python
import os
import sys
import numpy as np
import pandas as pd
import subprocess
import matplotlib as mpl
mpl.use('Agg')
import matplotlib.pyplot as plt
from matplotlib.legend_handler import HandlerLine2D
import itertools
from susipop import filter
from susipop import reader

import susipop as sp
from susipop.susi.cache import SusiCache as cache


phi = int(sys.argv[1])
gap = float(sys.argv[2])
shearrates = [float(i) for i in sys.argv[3:]]

def cluster_size_vs_cluster_probability_all_matplotlib(phi,shearrates,gap):
    s_data = np.asarray([])
    ns_data = np.asarray([])
    for shearrate in shearrates:
        try:
            path = "rhor1_phi"+str(phi)+"_yd"+str(float(shearrate))
            dataset = sp.reader.DataSet(path,particles=True, fluid=False)
            print path
            cache_file = cache(cachefile=dataset.cachefile)
            for i,t in enumerate(dataset.tlist[1:]):
                print path, int(float(i)/len(dataset.tlist[1:])*100)
                try:
                    if cache_file.has_data(t,"cluster_size_distribution_gap_"+str(gap)):
                        s, ns = cache_file.load_data(t,"cluster_size_distribution_gap_"+str(gap))
                        idx = np.where(ns != 0)[0]
                        s_data = np.concatenate([s_data , s[idx]])
                        ns_data = np.concatenate([ns_data , ns[idx]])
                    else:
                        dataset.load_state(t)
                        s, ns = dataset.get_quantity("cluster_size_distribution", gap = gap, update=True)
                        cache_file.save_data(t,{"cluster_size_distribution_gap_"+str(gap):[s, ns]})
                        idx = np.where(ns != 0)[0]
                        s_data = np.concatenate([s_data , s[idx]])
                        ns_data = np.concatenate([ns_data , ns[idx]])
                except Exception as e:
                    print e
            cache_file.close()
            del cache_file
        except Exception as e:
            print e
            continue
    uniq_sizes = np.unique(s_data)
    ns_data_cumulative = np.zeros_like(uniq_sizes)
    for i, item in enumerate(uniq_sizes):
        idx_uniq = np.where(s_data == item)[0]
        ns_data_cumulative[i] = sum(ns_data[idx_uniq])

#     x_new = np.linspace(x[0], x[-1], num=len(x)*10)
#     import numpy.polynomial.polynomial as poly
#     coefs = poly.polyfit(x, y, 20)
#     ffit = poly.polyval(x_new, coefs)
#     def zipfs(k,s,N):
#         denom = sum([1.0/i**s for i in range(1,int(N+1))])
#         num = 1.0/k**s
#         return num/denom


#     plt.subplot(3,1,1)
    x,y = uniq_sizes, ns_data_cumulative
    plt.style.use("ggplot")
    plt.figure()
    plt.loglog(x,  y/float(max(y)),'x')
    plt.loglog(x,  y/float(max(y)), linewidth=1.0,label="gap="+str(gap))
    #uncomment to enable zipf's law visualization
#     plt.loglog(range(1,int(max(x))),  [zipfs(i,alpha,max(x)) for i in range(1,int(max(x)))],label= "zipfs law")
#     plt.plot(x_new, ffit, 'r')
    plt.legend(loc="best")
    plt.xlim([0,1e3])
    plt.ylim([1e-7,2])
    plt.xlabel("N")
    plt.ylabel("P(N)")
    print uniq_sizes, ns_data_cumulative
    name = "phi"+str(phi)+"_yd_"
    for j in shearrates: name += str(j).replace('.','_')+"_"
    plt.suptitle("phi"+str(phi)+"_gap="+str(gap).replace(".","_"))
    plt.savefig(name+str(gap).replace(".","_"))    

cluster_size_vs_cluster_probability_all_matplotlib(phi,shearrates,gap)
# for i,item in enumerate(pdf_average.children):
#     print i, item.description 
# display(widget_cluster_size_distribution_matplotlib)
# display(widgets.HBox(cluster_size_distribution.children[4:]))
